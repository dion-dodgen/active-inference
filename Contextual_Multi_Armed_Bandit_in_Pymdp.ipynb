{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lu35jiGX4Iae"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dion-dodgen/active-inference/blob/main/Contextual_Multi_Armed_Bandit_in_Pymdp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-lrmYnLQ_r9"
      },
      "source": [
        "! pip install inferactively-pymdp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNEs7OkBaOl7"
      },
      "source": [
        "## **Imports some helpful libraries, most importantly `numpy` and `pymdp`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk0kXw1RRmTf"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pymdp\n",
        "from pymdp import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Lif_CBaZVO"
      },
      "source": [
        "### Let's set up the dimensionalities of the hidden state factors and the control states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGgFE62GnC-Y"
      },
      "source": [
        "\"\"\" Define dimensionalities of the hidden state factors and control state factors \"\"\"\n",
        "num_states = [3, 3] # a list of dimensionalities of each hidden state factor\n",
        "num_factors = len(num_states) # the total number of hidden state factors\n",
        "\n",
        "num_controls = [3, 3] # a list of the dimensionalities of each control state factor\n",
        "num_control_factors = len(num_controls) # the total number of control state factors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EamGUpb4IHU"
      },
      "source": [
        "\"\"\" Build an object array for storing the factor-specific B matrices \"\"\"\n",
        "# B ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu35jiGX4Iae"
      },
      "source": [
        "### *Solution*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn-BNOUq4I5T"
      },
      "source": [
        "B = utils.initialize_empty_B(num_states, num_controls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHlLECdDacE6"
      },
      "source": [
        "## Let's build the $\\mathbf{B}$ array, i.e. one $\\mathbf{B}$ matrix for each of the two hidden state factors of our new Grid World\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1aABVu57DxisLi9iTzcF0rngm8LQ1B5Uk\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_UrOs98nP-M"
      },
      "source": [
        "for f, ns in enumerate(num_states):\n",
        "\n",
        "  \"\"\" Initialize the B matrix for this factor \"\"\"\n",
        "  B[f] = np.zeros( (ns, ns, num_controls[f]))\n",
        "\n",
        "  # MOVE LEFT (or UP)\n",
        "  B[f][0,0:2,0] = 1.0\n",
        "  B[f][1,2,0] = 1.0\n",
        "\n",
        "  # MOVE RIGHT (or DOWN)\n",
        "  B[f][1,0,1] = 1.0\n",
        "  B[f][2,1:,1] = 1.0\n",
        "\n",
        "  # STAY\n",
        "  B[f][:,:,2] = np.eye(ns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIsnWZGASU9Z"
      },
      "source": [
        "### Plot the B matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNMfDL2C4T1q"
      },
      "source": [
        "utils.plot_likelihood(B[0][:,:,0], title = 'Move LEFT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-wceT0doO_A"
      },
      "source": [
        "## Now for the $\\mathbf{A}$ array..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5lsX1_-7eEZ"
      },
      "source": [
        "### Let's start by specifying a single observation modality - the observation of one's own location, or $o^{Loc}$.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cUkRY22yeIZdA6kyBO4mKB6jaXQeVpNo\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN4M_Dl57ZwV"
      },
      "source": [
        "num_obs = [9]\n",
        "num_modalities = len(num_obs)\n",
        "\n",
        "A = utils.initialize_empty_A(num_obs, num_states)\n",
        "A_location_dims = num_obs + num_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeBUAoaUTGm-"
      },
      "source": [
        "A[0] = np.zeros(A_location_dims)\n",
        "A[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6JsWem27zGh"
      },
      "source": [
        "### Now fill out the entries of the $\\mathbf{A}$ matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEzZH-WHe6Bc"
      },
      "source": [
        "\"\"\" filling out the mapping to 9-dimensional observations for the three possible settings of X (0, 1, 2), in the case we're in Y = 0 \"\"\"\n",
        "A[0][0:3,:,0] = np.eye(3)\n",
        "\n",
        "\"\"\" filling out the mapping to 9-dimensional observations for the three possible settings of X (0, 1, 2), in the case we're in Y = 1 \"\"\"\n",
        "A[0][3:6,:,1] = np.eye(3)\n",
        "\n",
        "\"\"\" filling out the mapping to 9-dimensional observations for the three possible settings of X (0, 1, 2), in the case we're in Y = 2 \"\"\"\n",
        "A[0][6:9,:,2] = np.eye(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN7_SrsQ7H7o"
      },
      "source": [
        "### Now display the $\\mathbf{A}$ matrices we just made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbhAmfh77uWN"
      },
      "source": [
        "utils.plot_likelihood(A[0][:,:,0], title = \"P(o | x, y == 0)\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vJTMZJN8Kt_"
      },
      "source": [
        "utils.plot_likelihood(A[0][:,:,1], title = \"P(o | x, y == 1)\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6lXE_A89Ulj"
      },
      "source": [
        "utils.plot_likelihood(A[0][:,:,2], title = \"P(o | x, y == 2)\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk0dbpXibpP_"
      },
      "source": [
        "# **Explore/exploit task with a contextual two-armed bandit**\n",
        "\n",
        "## Now we're going to build a generative model for an active inference agent playing a two-armed bandit task. The [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) is a classic decision-making task that captures the core features of the the \"explore/exploit tradeoff\". This problem formulation is ubiqutious across various disciplines that study decision-making under uncertainty, including economics, neuroscience, machine learning, and engineering.\n",
        "\n",
        "## The agent has to make choices among mutually exclusive alternatives or 'arms' in order maximize rewards, which issue probabilistically from each arm. However, the reward statistics of each arm are in general unknown or only partially known. The agent must therefore _infer_ the reward statistics.\n",
        "\n",
        "## The inherently partial-observability  of the ask creates a conflict between **exploitation** or choosing the arm that is _currently believed_ to be most rewarding, and **exploration** or gathering information about the remaining arms, in the hopes of discovering a potentially more rewarding option.\n",
        "\n",
        "## The fact that expected reward or utility is contextualized by _beliefs_ -- i.e. which arm is currently thought to be the most rewarding -- motivates the use of active inference in this context. This is because the key objective function for action-selection, the expected free energy $\\mathbf{G}$, depends on the agent's beliefs about the world. And not only that, but expected free energy balances the desire to maximize rewards with the drive to resolve uncertainty about unknown parts of the agent's model. The more accurate the agent's beliefs are, the more faithfully decision-making can be guided by maximizing expected utility or rewards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYTRxlxx4iD0"
      },
      "source": [
        "### Specify the dimensionalities of the hidden state factors, the control factors, and the observation modalities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAZ_g0UmdEVj"
      },
      "source": [
        "context_names = ['Left-Better', 'Right-Better']\n",
        "choice_names = ['Start', 'Hint', 'Left Arm', 'Right Arm']\n",
        "\n",
        "\"\"\" Define `num_states` and `num_factors` below \"\"\"\n",
        "num_states = [len(context_names), len(choice_names)]\n",
        "num_factors = len(num_states)\n",
        "\n",
        "context_action_names = ['Do-nothing']\n",
        "choice_action_names = ['Move-start', 'Get-hint', 'Play-left', 'Play-right']\n",
        "\n",
        "\"\"\" Define `num_controls` below \"\"\"\n",
        "num_controls = [len(context_action_names), len(choice_action_names)]\n",
        "\n",
        "hint_obs_names = ['Null', 'Hint-left', 'Hint-right']\n",
        "reward_obs_names = ['Null', 'Loss', 'Reward']\n",
        "choice_obs_names = ['Start', 'Hint', 'Left Arm', 'Right Arm']\n",
        "\n",
        "\"\"\" Define `num_obs` and `num_modalities` below \"\"\"\n",
        "num_obs = [len(hint_obs_names), len(reward_obs_names), len(choice_obs_names)]\n",
        "num_modalities = len(num_obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Tca2bJ4o0z"
      },
      "source": [
        "## Create the $\\mathbf{A}$ arrays first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQsDxB8v6bvF"
      },
      "source": [
        "\"\"\" Generate the A array \"\"\"\n",
        "A = utils.initialize_empty_A(num_obs, num_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vmy_Z1N6poY"
      },
      "source": [
        "## Fill out the hint modality, a sub-array of `A` which we'll call `A_hint`\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1SqMp77NAmUa_oh925VURJ1Hyp8v_fXOj\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daovgh4A6m5c"
      },
      "source": [
        "p_hint = 0.7 # accuracy of the hint, according to the agent's generative model (how much does the agent trust the hint?)\n",
        "\n",
        "A_hint = np.zeros( (len(hint_obs_names), len(context_names), len(choice_names)) )\n",
        "\n",
        "for choice_id, choice_name in enumerate(choice_names):\n",
        "\n",
        "  if choice_name == 'Start':\n",
        "\n",
        "    A_hint[0,:,choice_id] = 1.0\n",
        "\n",
        "  elif choice_name == 'Hint':\n",
        "\n",
        "    A_hint[1:,:,choice_id] = np.array([[p_hint,       1.0 - p_hint],\n",
        "                                      [1.0 - p_hint,  p_hint]])\n",
        "  elif choice_name == 'Left Arm':\n",
        "\n",
        "    A_hint[0,:,choice_id] = 1.0\n",
        "\n",
        "  elif choice_name == 'Right Arm':\n",
        "\n",
        "    A_hint[0,:,choice_id] = 1.0\n",
        "\n",
        "A[0] = A_hint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIWcFelA7RqY"
      },
      "source": [
        "utils.plot_likelihood(A[0][:,:,1], title = \"Probability of the two hint types, for the two game states\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnVJO-Nn7zY4"
      },
      "source": [
        "## Fill out the reward modality, a sub-array of `A` which we'll call `A_rew`\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=155LAPZ9_aulJ3YYZwwlOWrEa6unabHht\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVnKm0zU8Ng3"
      },
      "source": [
        "p_reward = 0.8 # probability of getting a rewarding outcome, if you are sampling the more rewarding bandit\n",
        "\n",
        "A_reward = np.zeros((len(reward_obs_names), len(context_names), len(choice_names)))\n",
        "\n",
        "for choice_id, choice_name in enumerate(choice_names):\n",
        "\n",
        "  if choice_name == 'Start':\n",
        "\n",
        "    A_reward[0,:,choice_id] = 1.0\n",
        "\n",
        "  elif choice_name == 'Hint':\n",
        "\n",
        "    A_reward[0,:,choice_id] = 1.0\n",
        "\n",
        "  elif choice_name == 'Left Arm':\n",
        "\n",
        "    A_reward[1:,:,choice_id] = np.array([ [1.0-p_reward, p_reward],\n",
        "                                        [p_reward, 1.0-p_reward]])\n",
        "  elif choice_name == 'Right Arm':\n",
        "\n",
        "    A_reward[1:, :, choice_id] = np.array([[ p_reward, 1.0- p_reward],\n",
        "                                         [1- p_reward, p_reward]])\n",
        "\n",
        "A[1] = A_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWKQSusN9Jzk"
      },
      "source": [
        "utils.plot_likelihood(A[1][:,:,2], title='Payoff structure if playing the Left Arm, for the two contexts')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g62Zjwg0-iot"
      },
      "source": [
        "## Fill out the choice observation modality, a sub-array of `A` which we'll call `A_choice`\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1LGdGX0TgesvQ2HDnHMg42XHh0n6ZKnHw\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdgBVSm-mOL"
      },
      "source": [
        "A_choice = np.zeros((len(choice_obs_names), len(context_names), len(choice_names)))\n",
        "\n",
        "for choice_id in range(len(choice_names)):\n",
        "\n",
        "  A_choice[choice_id, :, choice_id] = 1.0\n",
        "\n",
        "A[2] = A_choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O5QpoC8Uc8j"
      },
      "source": [
        "\"\"\" Condition on context (first hidden state factor) and display the remaining indices (outcome and choice state) \"\"\"\n",
        "\n",
        "utils.plot_likelihood(A[2][:,0,:], title=\"Mapping between sensed states and true states\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's wrap that all into a single function so we can quickly re-set or re-parameterize the A array as needed"
      ],
      "metadata": {
        "id": "mjRkZ9JiUOOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_A(p_hint=0.7, p_reward=0.8):\n",
        "  \"\"\"\n",
        "  Function for creating the observation (observation or sensory likelihood) model for the contextual MAB task, parameterized by\n",
        "  two probabilities: `p_hint` and `p_reward`.\n",
        "  Parameters:\n",
        "  ----------\n",
        "  `p_hint`: float (default 0.7)\n",
        "    accuracy of the hint, according to the agent's generative model (how much does the agent trust the hint?)\n",
        "  `p_reward`: float (default 0.8)\n",
        "    probability of getting a rewarding outcome when sampling the more rewarding bandit, according to the agent's generative model\n",
        "  Returns:\n",
        "  ---------\n",
        "  `A`: numpy object array\n",
        "    The full observation likelihood model\n",
        "  \"\"\"\n",
        "  A = utils.initialize_empty_A(num_obs, num_states)\n",
        "\n",
        "  \"\"\" Fill out the hint modality \"\"\"\n",
        "  A_hint = np.zeros( (len(hint_obs_names), len(context_names), len(choice_names)) )\n",
        "  for choice_id, choice_name in enumerate(choice_names):\n",
        "    if choice_name == 'Start':\n",
        "      A_hint[0,:,choice_id] = 1.0\n",
        "    elif choice_name == 'Hint':\n",
        "      A_hint[1:,:,choice_id] = np.array([[p_hint,       1.0 - p_hint],\n",
        "                                        [1.0 - p_hint,  p_hint]])\n",
        "    elif choice_name == 'Left Arm':\n",
        "      A_hint[0,:,choice_id] = 1.0\n",
        "    elif choice_name == 'Right Arm':\n",
        "      A_hint[0,:,choice_id] = 1.0\n",
        "\n",
        "  \"\"\" Fill out the reward modality \"\"\"\n",
        "  A_reward = np.zeros((len(reward_obs_names), len(context_names), len(choice_names)))\n",
        "  for choice_id, choice_name in enumerate(choice_names):\n",
        "    if choice_name == 'Start':\n",
        "      A_reward[0,:,choice_id] = 1.0\n",
        "    elif choice_name == 'Hint':\n",
        "      A_reward[0,:,choice_id] = 1.0\n",
        "    elif choice_name == 'Left Arm':\n",
        "      A_reward[1:,:,choice_id] = np.array([ [1.0-p_reward, p_reward],\n",
        "                                          [p_reward, 1.0-p_reward]])\n",
        "    elif choice_name == 'Right Arm':\n",
        "      A_reward[1:, :, choice_id] = np.array([[ p_reward, 1.0- p_reward],\n",
        "                                          [1- p_reward, p_reward]])\n",
        "  \"\"\" Fill out the choice sensation modality \"\"\"\n",
        "  A_choice = np.zeros((len(choice_obs_names), len(context_names), len(choice_names)))\n",
        "  for choice_id in range(len(choice_names)):\n",
        "      A_choice[choice_id, :, choice_id] = 1.0\n",
        "\n",
        "  A[0], A[1], A[2] = A_hint, A_reward, A_choice\n",
        "\n",
        "  return A"
      ],
      "metadata": {
        "id": "gKuY_N5KUNtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhP5apgR-7zN"
      },
      "source": [
        "## Now let's move onto the $\\mathbf{B}$ arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSiiIJdu4n9b"
      },
      "source": [
        "B = utils.initialize_empty_B(num_states, num_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b7Fyz0T59e0"
      },
      "source": [
        "### Fill out the context state factor dynamics, a sub-array of `B` which we'll call `B_context`\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1_VvkCpRu1wWwEFiAJKnOGAGikd5KeiiE\" width=\"600\" height=\"300\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVtIXiyq6EHo"
      },
      "source": [
        "B_context = np.zeros( (len(context_names), len(context_names), len(context_action_names)) )\n",
        "\n",
        "B_context[:,:,0] = np.eye(len(context_names))\n",
        "\n",
        "B[0] = B_context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzDc6PLG5Mof"
      },
      "source": [
        "### Fill out the choice factor dynamics, a sub-array of `B` which we'll call `B_choice`\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1qeuFvNIrJR7ldjpkrB6_jAp6JM3UhMw0\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--5HyEYt42Q8"
      },
      "source": [
        "B_choice = np.zeros( (len(choice_names), len(choice_names), len(choice_action_names)) )\n",
        "\n",
        "for choice_i in range(len(choice_names)):\n",
        "\n",
        "  B_choice[choice_i, :, choice_i] = 1.0\n",
        "\n",
        "B[1] = B_choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Once again, let's wrap that into a quick `create_B()` function so we can re-set it whenever we want.\n",
        "\n",
        "Let's add in an optional `change_prob` parameter to allow the agent to believe in a stochastic environment (the \"better arm\" may change identity over time)"
      ],
      "metadata": {
        "id": "bNMrCkMwaJ_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_B(p_change=0.0):\n",
        "  \"\"\"\n",
        "  Function for creating the transition (dynamics or transition likelihood) model for the contextual MAB task, parameterized by\n",
        "  a context-change probability `p_change`.\n",
        "  Parameters:\n",
        "  ----------\n",
        "  `p_change`: float (default 1.0)\n",
        "    probability of the context (which bandit is more rewarding) changing\n",
        "  Returns:\n",
        "  ---------\n",
        "  `B`: numpy object array\n",
        "    The full transition likelihood model\n",
        "  \"\"\"\n",
        "  B = utils.initialize_empty_B(num_states, num_states)\n",
        "\n",
        "  \"\"\" Context transitions (uncontrollable) \"\"\"\n",
        "  B_context = np.zeros( (len(context_names), len(context_names), len(context_action_names)) )\n",
        "  B_context[:,:,0] = np.array([[1.-p_change,    p_change],\n",
        "                               [p_change, 1.-p_change]]\n",
        "                              )\n",
        "\n",
        "  \"\"\" Choice transitions (controllable) \"\"\"\n",
        "  B_choice = np.zeros( (len(choice_names), len(choice_names), len(choice_action_names)) )\n",
        "  for choice_i in range(len(choice_names)):\n",
        "    B_choice[choice_i, :, choice_i] = 1.0\n",
        "\n",
        "  B[0], B[1] = B_context, B_choice\n",
        "\n",
        "  return B\n"
      ],
      "metadata": {
        "id": "ShiHfezPaJlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll-mBL_2_McK"
      },
      "source": [
        "## The $\\mathbf{C}$ vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JlKcepO_KgA"
      },
      "source": [
        "from pymdp.maths import softmax\n",
        "\n",
        "def create_C(reward=2., pun=-4.):\n",
        "  \"\"\" Creates the C array, AKA the observation prior for the MAB task, parameterized by a `reward` and `pun` (punishment) parameter \"\"\"\n",
        "\n",
        "  C = utils.obj_array_zeros(num_obs)\n",
        "  C[1] = np.array([0., pun, reward])\n",
        "  return C\n",
        "\n",
        "C = create_C(reward=2.0, pun=-4.0)\n",
        "\n",
        "utils.plot_beliefs(softmax(C[1]), title = \"Prior preferences\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJXKG-IV_qm1"
      },
      "source": [
        "## The $\\mathbf{D}$ vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJTKm27fv_Rf"
      },
      "source": [
        "def create_D(p_context=0.5):\n",
        "  \"\"\"\n",
        "  Creates the D array AKA the hidden state prior at the first timestep for the MAB task, parameterized by a `p_context` parameter that\n",
        "  parameterizes the agent's prior beliefs about whether the context is \"Left Arm Better\" at the first timestep of a given trial\n",
        "  \"\"\"\n",
        "\n",
        "  D = utils.obj_array(num_factors)\n",
        "\n",
        "  \"\"\" Context prior \"\"\"\n",
        "  D_context = np.array([p_context,1.-p_context])\n",
        "  D[0] = D_context\n",
        "\n",
        "\n",
        "  \"\"\" Choice-state prior \"\"\"\n",
        "  D_choice = np.zeros(len(choice_names))\n",
        "  D_choice[choice_names.index(\"Start\")] = 1.0\n",
        "  D[1] = D_choice\n",
        "\n",
        "  return D\n",
        "\n",
        "D = create_D()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utils.plot_beliefs(softmax(D[0]), title = \"Prior beliefs about probability of the two contexts\")"
      ],
      "metadata": {
        "id": "7Txj8Nc_esKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP7NySzAhWgf"
      },
      "source": [
        "## Now let's take advantage of the `Agent` class in `pymdp` to wrap this all into an Agent instance that we can use to do active inference in a few lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls9VoZX__0Vp"
      },
      "source": [
        "from pymdp.agent import Agent\n",
        "\n",
        "A = create_A(p_hint=0.7, p_reward=0.8)\n",
        "B = create_B(p_change=0.0)\n",
        "C = create_C(reward=2.0, pun=-4.0)\n",
        "D = create_D(p_context=0.5)\n",
        "my_agent = Agent(A=A, B=B, C=C, D=D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xu-ZVZBh_wH"
      },
      "source": [
        "## Define a class for the 2-armed bandit environment (AKA the _generative process_)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjxoLs67AXwR"
      },
      "source": [
        "class TwoArmedBandit(object):\n",
        "\n",
        "  def __init__(self, context = None, p_hint = 1.0, p_reward = 0.8):\n",
        "\n",
        "    self.context_names = [\"Left-Better\", \"Right-Better\"]\n",
        "\n",
        "    if context == None:\n",
        "      self.context = self.context_names[utils.sample(np.array([0.5, 0.5]))] # randomly sample which bandit arm is better (Left or Right)\n",
        "    else:\n",
        "      self.context = context\n",
        "\n",
        "    self.p_hint = p_hint\n",
        "    self.p_reward = p_reward\n",
        "\n",
        "    self.reward_obs_names = ['Null', 'Loss', 'Reward']\n",
        "    self.hint_obs_names = ['Null', 'Hint-left', 'Hint-right']\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    if action == \"Move-start\":\n",
        "      observed_hint = \"Null\"\n",
        "      observed_reward = \"Null\"\n",
        "      observed_choice = \"Start\"\n",
        "    elif action == \"Get-hint\":\n",
        "      if self.context == \"Left-Better\":\n",
        "        observed_hint = self.hint_obs_names[utils.sample(np.array([0.0, self.p_hint, 1.0 - self.p_hint]))]\n",
        "      elif self.context == \"Right-Better\":\n",
        "        observed_hint = self.hint_obs_names[utils.sample(np.array([0.0, 1.0 - self.p_hint, self.p_hint]))]\n",
        "      observed_reward = \"Null\"\n",
        "      observed_choice = \"Hint\"\n",
        "    elif action == \"Play-left\":\n",
        "      observed_hint = \"Null\"\n",
        "      observed_choice = \"Left Arm\"\n",
        "      if self.context == \"Left-Better\":\n",
        "        observed_reward = self.reward_obs_names[utils.sample(np.array([0.0, 1.0 - self.p_reward, self.p_reward]))]\n",
        "      elif self.context == \"Right-Better\":\n",
        "        observed_reward = self.reward_obs_names[utils.sample(np.array([0.0, self.p_reward, 1.0 - self.p_reward]))]\n",
        "    elif action == \"Play-right\":\n",
        "      observed_hint = \"Null\"\n",
        "      observed_choice = \"Right Arm\"\n",
        "      if self.context == \"Right-Better\":\n",
        "        observed_reward = self.reward_obs_names[utils.sample(np.array([0.0, 1.0 - self.p_reward, self.p_reward]))]\n",
        "      elif self.context == \"Left-Better\":\n",
        "        observed_reward = self.reward_obs_names[utils.sample(np.array([0.0, self.p_reward, 1.0 - self.p_reward]))]\n",
        "\n",
        "    obs = [observed_hint, observed_reward, observed_choice]\n",
        "\n",
        "    return obs\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6IeX_BlhmBy"
      },
      "source": [
        "### Write a function that will take the agent, the environment, and a time length and run the active inference loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In-AoaGQDdQH"
      },
      "source": [
        "def run_active_inference_loop(my_agent, my_env, T = 5, verbose = False):\n",
        "  \"\"\"\n",
        "  Function that wraps together and runs a full active inference loop using the pymdp.agent.Agent class functionality\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\" Initialize the first observation \"\"\"\n",
        "  obs_label = [\"Null\", \"Null\", \"Start\"]  # agent observes itself seeing a `Null` hint, getting a `Null` reward, and seeing itself in the `Start` location\n",
        "  obs = [hint_obs_names.index(obs_label[0]), reward_obs_names.index(obs_label[1]), choice_obs_names.index(obs_label[2])]\n",
        "\n",
        "  first_choice = choice_obs_names.index(obs_label[2])\n",
        "  choice_hist = np.zeros((4,T+1))\n",
        "  choice_hist[first_choice,0] = 1.0\n",
        "\n",
        "  belief_hist = np.zeros((2, T))\n",
        "  context_hist = np.zeros(T)\n",
        "\n",
        "  for t in range(T):\n",
        "    context_hist[t] = env.context_names.index(env.context)\n",
        "    qs = my_agent.infer_states(obs)\n",
        "\n",
        "    belief_hist[:,t] = qs[0]\n",
        "\n",
        "    if verbose:\n",
        "      utils.plot_beliefs(qs[0], title = f\"Beliefs about the context at time {t}\")\n",
        "\n",
        "    q_pi, efe = my_agent.infer_policies()\n",
        "    chosen_action_id = my_agent.sample_action()\n",
        "\n",
        "    movement_id = int(chosen_action_id[1])\n",
        "    choice_hist[movement_id,t+1]= 1.0\n",
        "\n",
        "    choice_action = choice_action_names[movement_id]\n",
        "\n",
        "    obs_label = my_env.step(choice_action)\n",
        "\n",
        "    obs = [hint_obs_names.index(obs_label[0]), reward_obs_names.index(obs_label[1]), choice_obs_names.index(obs_label[2])]\n",
        "\n",
        "    if verbose:\n",
        "      print(f'Action at time {t}: {choice_action}')\n",
        "      print(f'Reward at time {t}: {obs_label[1]}')\n",
        "\n",
        "  return choice_hist, belief_hist, context_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_choices_beliefs(choice_hist, belief_hist, context_hist, pad_val=5.0):\n",
        "  \"\"\" Helper function for plotting outcome of simulation.\n",
        "  first subplot shows the agent's choices (actions) over time , second subplot shows the agents beliefs about the game-context (which arm is better) over time\n",
        "  \"\"\"\n",
        "\n",
        "  T = choice_hist.shape[1]\n",
        "  fig, axes = plt.subplots(nrows = 2, ncols = 1, figsize = (14,11))\n",
        "  axes[0].imshow(choice_hist[:,:-1], cmap = 'gray') # only plot up until the second to last timestep, since we don't update beliefs after the last choice\n",
        "  axes[0].set_xlabel('Timesteps')\n",
        "  axes[0].set_yticks(ticks = range(4))\n",
        "  axes[0].set_yticklabels(labels = choice_action_names)\n",
        "  axes[0].set_title('Choices over time')\n",
        "\n",
        "  axes[1].imshow(belief_hist, cmap = 'gray')\n",
        "  axes[1].set_xlabel('Timesteps')\n",
        "  axes[1].set_yticks(ticks = range(2))\n",
        "  axes[1].set_yticklabels(labels = ['Left-Better', 'Right-Better'])\n",
        "  axes[1].set_title('Beliefs over time')\n",
        "  axes[1].scatter(np.arange(T-1), context_hist, c = 'r', s = 50)\n",
        "\n",
        "  fig.tight_layout(pad=pad_val)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2vYrtK8Mbi9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYeg73orwtHC"
      },
      "source": [
        "### Now all we have to do is define the two-armed bandit environment, choose the length of the simulation, and run the function we wrote above.\n",
        "\n",
        "\n",
        "*   Try playing with the hint accuracy and/or reward statistics of the environment - remember this is _different_ than the agent's representation of the reward statistics (i.e. the agent's generative model, e.g. the A or B matrices).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy7OXyo8wvY0"
      },
      "source": [
        "p_hint_env = 1.0 # this is the \"true\" accuracy of the hint - i.e. how often does the hint actually signal which arm is better. REMEMBER: THIS IS INDEPENDENT OF HOW YOU PARAMETERIZE THE A MATRIX FOR THE HINT MODALITY\n",
        "p_reward_env = 0.7 # this is the \"true\" reward probability - i.e. how often does the better arm actually return a reward, as opposed to a loss. REMEMBER: THIS IS INDEPENDENT OF HOW YOU PARAMETERIZE THE A MATRIX FOR THE REWARD MODALITY\n",
        "env = TwoArmedBandit(p_hint=p_hint_env, p_reward=p_reward_env)\n",
        "\n",
        "T = 15\n",
        "\n",
        "A = create_A(p_hint=0.7, p_reward=0.8)\n",
        "B = create_B(p_change=0.0)\n",
        "C = create_C(reward=2.0, pun=-4.0)\n",
        "D = create_D(p_context=0.5)\n",
        "my_agent = Agent(A = A, B = B, C = C, D = D) # in case you want to re-define the agent, you can run this again\n",
        "choice_hist, belief_hist, context_hist = run_active_inference_loop(my_agent, env, T = T, verbose = False)\n",
        "plot_choices_beliefs(choice_hist, belief_hist, context_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scuxk92i3Mef"
      },
      "source": [
        "### Let's manipulate the agent's prior preferences over reward observations ($\\mathbf{C}[1]$) in order to examine the tension between exploration and exploitation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1XpcHlX3Gnj"
      },
      "source": [
        "# manipulate the agent's sensitivity to punishment\n",
        "C = create_C(pun=-4.0)\n",
        "my_agent = Agent(A = A, B = B, C = C, D = D) # redefine the agent with the new preferences\n",
        "env = TwoArmedBandit(p_hint = 1.0, p_reward = 0.7) # re-initialize the environment)\n",
        "\n",
        "choice_hist, belief_hist, context_hist = run_active_inference_loop(my_agent, env, T = T, verbose = False)\n",
        "plot_choices_beliefs(choice_hist, belief_hist, context_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning the reward contingencies"
      ],
      "metadata": {
        "id": "lTc4pnZ3KE8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining priors over the (Categorical) parameters of the `A` array. A natural choice of the prior is the Dirichlet distribution, which is conjugate to the Categorical likelihood."
      ],
      "metadata": {
        "id": "5g9ZdY7DfZQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = create_A(p_hint=1.0, p_reward=0.51) # let's assume the agent doesn't know the reward contingencies\n",
        "pA = utils.dirichlet_like(A, scale = 0.1)"
      ],
      "metadata": {
        "id": "8grHH_A-fkf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Often we want certain contingencies to be unavailable to learning -- these are contingencies that we assume are \"baked-in\" to the agent's beliefs about the world and not adaptable."
      ],
      "metadata": {
        "id": "H7RGmWN6fosX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pA[1][0,:,:] *= 10e5 # make the null observation contingencies 'un-learnable'"
      ],
      "metadata": {
        "id": "PUgYTkSBfn2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's make that into a function to enable quick re-parameterization of `pA`"
      ],
      "metadata": {
        "id": "mGuDcjjfk6bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parameterize_pA(A_base, scale=1e-16, prior_count=10e5):\n",
        "  pA = utils.dirichlet_like(A_base, scale = scale)\n",
        "  pA[1][0,:,:] *= prior_count # make the null observation contingencies 'un-learnable'\n",
        "  return pA"
      ],
      "metadata": {
        "id": "VDcFB5HNk8cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define new active inference loop where you update beliefs about A (`qA`) online and save the results"
      ],
      "metadata": {
        "id": "hylBipXEjyFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_active_inference_with_learning(my_agent, my_env, T = 5):\n",
        "  \"\"\"\n",
        "  Function that wraps together and runs a full active inference loop using the pymdp.agent.Agent class functionality.\n",
        "  Also includes learning and outputs the history of the agent's beliefs about the reward\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\" Initialize the first observation \"\"\"\n",
        "  obs_label = [\"Null\", \"Null\", \"Start\"]  # agent observes itself seeing a `Null` hint, getting a `Null` reward, and seeing itself in the `Start` location\n",
        "  obs = [hint_obs_names.index(obs_label[0]), reward_obs_names.index(obs_label[1]), choice_obs_names.index(obs_label[2])]\n",
        "\n",
        "  belief_hist = np.zeros((2, T))\n",
        "  context_hist = np.zeros(T)\n",
        "\n",
        "  first_choice = choice_obs_names.index(obs_label[2])\n",
        "  choice_hist = np.zeros((4,T+1))\n",
        "  choice_hist[first_choice,0] = 1.0\n",
        "\n",
        "  dim_qA = (T,) + my_agent.A[my_agent.modalities_to_learn[0]].shape\n",
        "\n",
        "  qA_hist = np.zeros(dim_qA)\n",
        "\n",
        "  for t in range(T):\n",
        "    context_hist[t] = env.context_names.index(env.context)\n",
        "    qs = my_agent.infer_states(obs)\n",
        "    belief_hist[:,t] = qs[0].copy()\n",
        "\n",
        "    q_pi, _ = my_agent.infer_policies()\n",
        "    chosen_action_id = my_agent.sample_action()\n",
        "\n",
        "    movement_id = int(chosen_action_id[1])\n",
        "    choice_hist[movement_id,t+1]= 1.0\n",
        "\n",
        "    qA_t = my_agent.update_A(obs)\n",
        "    qA_hist[t] = qA_t[my_agent.modalities_to_learn[0]]\n",
        "\n",
        "    choice_action = choice_action_names[movement_id]\n",
        "    obs_label = my_env.step(choice_action)\n",
        "\n",
        "    # print(f'Observation : Hint: {obs_label[0]}, Reward: {obs_label[1]}, Choice Sense: {obs_label[2]}')\n",
        "    obs = [hint_obs_names.index(obs_label[0]), reward_obs_names.index(obs_label[1]), choice_obs_names.index(obs_label[2])]\n",
        "\n",
        "  return choice_hist, belief_hist, qA_hist, context_hist"
      ],
      "metadata": {
        "id": "2Y0fT9IFkQBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = create_A(p_hint=1.0, p_reward=0.51) # let's assume the agent doesn't know the reward contingencies\n",
        "pA = parameterize_pA(A_base=A, scale = 1.0, prior_count=10e5)\n",
        "\n",
        "B, C, D = create_B(), create_C(reward=2., pun=-4.), create_D() # the rest of the generative model\n",
        "env = TwoArmedBandit(p_hint = 1.0, p_reward = 0.8) # initialize the environment with p_reward = 0.7\n",
        "\n",
        "agent_with_learning = Agent(A=A, pA=pA, B=B, C=C, D=D, modalities_to_learn=[1], lr_pA = 1.0, use_param_info_gain=True, action_selection='deterministic')\n",
        "T = 25\n",
        "choice_hist, belief_hist, qA_hist, context_hist = run_active_inference_with_learning(agent_with_learning, env, T = T)\n",
        "plot_choices_beliefs(choice_hist, belief_hist, context_hist, pad_val = 0.1)"
      ],
      "metadata": {
        "id": "KnGahHvIrAGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_reward_beliefs_left = [utils.norm_dist(qa_t)[2,0,2] for qa_t in qA_hist]\n",
        "p_reward_beliefs_right = [utils.norm_dist(qa_t)[2,1,3] for qa_t in qA_hist]\n",
        "\n",
        "print(f'True context is: {env.context}')\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "ax.plot(p_reward_beliefs_left, label = 'Beliefs about $p_{reward}$ when Left Arm is better', lw = 2.0)\n",
        "ax.plot(p_reward_beliefs_right, label = 'Beliefs about $p_{reward}$ when Right Arm is better', lw = 2.0)\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "ax.set_xlim(0, T)\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "aBcI6xtX0Jv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In order to allow the agent to sample both bandit arms more equally (so it can learn the reward probabilities in the _other_ arm, not just the best arm given the context at hand), we will introduce a switching probability into the bandit by augmenting the bandit class so that the context sometimes stochastically switches."
      ],
      "metadata": {
        "id": "Pb2AlKpFzOb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoArmedBanditStochastic(object):\n",
        "\n",
        "  def __init__(self, context = None, p_hint = 1.0, p_reward = 0.8, p_change = 0.3):\n",
        "\n",
        "    self.context_names = [\"Left-Better\", \"Right-Better\"]\n",
        "\n",
        "    if context == None:\n",
        "      self.context = self.context_names[utils.sample(np.array([0.5, 0.5]))] # randomly sample which bandit arm is better (Left or Right)\n",
        "    else:\n",
        "      self.context = context\n",
        "\n",
        "    self.p_hint = p_hint\n",
        "    self.p_reward = p_reward\n",
        "\n",
        "    self.reward_obs_names = ['Null', 'Loss', 'Reward']\n",
        "    self.hint_obs_names = ['Null', 'Hint-left', 'Hint-right']\n",
        "\n",
        "    self.p_change=p_change\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    # change the context stochastically at each timestep\n",
        "    change_or_stay = utils.sample(np.array([self.p_change, 1. - self.p_change]))\n",
        "    if change_or_stay == 0:\n",
        "      if self.context == 'Left-Better':\n",
        "        self.context = 'Right-Better'\n",
        "      elif self.context == 'Right-Better':\n",
        "        self.context = 'Left-Better'\n",
        "\n",
        "    if action == \"Move-start\":\n",
        "      observed_hint = \"Null\"\n",
        "      observed_reward = \"Null\"\n",
        "      observed_choice = \"Start\"\n",
        "    elif action == \"Get-hint\":\n",
        "      if self.context == \"Left-Better\":\n",
        "        observed_hint = self.hint_obs_names[utils.sample(np.array([0.0, self.p_hint, 1.0 - self.p_hint]))]\n",
        "      elif self.context == \"Right-Better\":\n",
        "        observed_hint = self.hint_obs_names[utils.sample(np.array([0.0, 1.0 - self.p_hint, self.p_hint]))]\n",
        "      observed_reward = \"Null\"\n",
        "      observed_choice = \"Hint\"\n",
        "    elif action == \"Play-left\":\n",
        "      observed_hint = \"Null\"\n",
        "      observed_choice = \"Left Arm\"\n",
        "      if self.context == \"Left-Better\":\n",
        "        observed_reward = self.reward_obs_names[utils.sample(np.array([0.0, 1.0 - self.p_reward, self.p_reward]))]\n",
        "      elif self.context == \"Right-Better\":\n",
        "        observed_reward = self.reward_obs_names[utils.sample(np.array([0.0, self.p_reward, 1.0 - self.p_reward]))]\n",
        "    elif action == \"Play-right\":\n",
        "      observed_hint = \"Null\"\n",
        "      observed_choice = \"Right Arm\"\n",
        "      if self.context == \"Right-Better\":\n",
        "        observed_reward = self.reward_obs_names[utils.sample(np.array([0.0, 1.0 - self.p_reward, self.p_reward]))]\n",
        "      elif self.context == \"Left-Better\":\n",
        "        observed_reward = self.reward_obs_names[utils.sample(np.array([0.0, self.p_reward, 1.0 - self.p_reward]))]\n",
        "\n",
        "    obs = [observed_hint, observed_reward, observed_choice]\n",
        "\n",
        "    return obs\n",
        ""
      ],
      "metadata": {
        "id": "PsYMJL1pzNk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TwoArmedBanditStochastic(p_hint = 1.0, p_reward = 0.8, p_change=0.2) # initialize the environment with p_reward = 0.7\n",
        "print(f'Starting context is :{env.context}')\n",
        "\n",
        "A = create_A(p_hint=1.0, p_reward=0.51)\n",
        "pA = parameterize_pA(A_base=A, scale=0.05, prior_count=10e5)\n",
        "\n",
        "B = create_B(p_change=0.2)\n",
        "C = create_C(reward=2.0, pun=-2.0)\n",
        "D = create_D()\n",
        "\n",
        "T = 20\n",
        "agent_with_learning = Agent(A=A, pA=pA, B=B, C=C, D=D, modalities_to_learn=[1], lr_pA = 1.25, use_param_info_gain = True, action_selection='stochastic')\n",
        "choice_hist, belief_hist, qA_hist, context_hist = run_active_inference_with_learning(agent_with_learning, env, T = T)\n",
        "plot_choices_beliefs(choice_hist, belief_hist, context_hist, pad_val = 1.)"
      ],
      "metadata": {
        "id": "1CqRBn2a1vzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_reward_beliefs_left = [utils.norm_dist(qa_t)[2,0,2] for qa_t in qA_hist]\n",
        "p_reward_beliefs_right = [utils.norm_dist(qa_t)[2,1,3] for qa_t in qA_hist]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "ax.plot(p_reward_beliefs_left, label = 'Beliefs about $p_{reward}$ when Left Arm is better', lw = 2.0)\n",
        "ax.plot(p_reward_beliefs_right, label = 'Beliefs about $p_{reward}$ when Right Arm is better', lw = 2.0)\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "ax.set_xlim(0, T)\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "aGYjrXv-Z4sM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}